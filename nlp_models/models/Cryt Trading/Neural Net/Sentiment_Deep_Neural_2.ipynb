{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trax in /opt/conda/lib/python3.7/site-packages (1.4.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from trax) (1.14.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from trax) (1.21.6)\n",
      "Requirement already satisfied: gin-config in /opt/conda/lib/python3.7/site-packages (from trax) (0.5.0)\n",
      "Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.7/site-packages (from trax) (2.11.0)\n",
      "Requirement already satisfied: jaxlib in /opt/conda/lib/python3.7/site-packages (from trax) (0.3.25)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from trax) (1.7.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from trax) (5.6.7)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from trax) (3.1.3)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from trax) (4.8.2)\n",
      "Requirement already satisfied: gym in /opt/conda/lib/python3.7/site-packages (from trax) (0.26.2)\n",
      "Requirement already satisfied: funcsigs in /opt/conda/lib/python3.7/site-packages (from trax) (1.0.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from trax) (1.4.0)\n",
      "Requirement already satisfied: jax in /opt/conda/lib/python3.7/site-packages (from trax) (0.3.25)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from gym->trax) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym->trax) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/conda/lib/python3.7/site-packages (from gym->trax) (6.3.0)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax->trax) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jax->trax) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->trax) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->trax) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->trax) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->trax) (2.8.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (7.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (1.11.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (4.42.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (0.3.6)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (0.1.8)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (2.28.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (2.2.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (3.19.6)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (5.12.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (0.10.2)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (0.9.0)\n",
      "Requirement already satisfied: tensorflow<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text->trax) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text->trax) (0.13.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.7/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (3.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->trax) (59.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.8)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.32.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (23.3.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (20.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.11.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.11.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.54.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (16.0.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.59.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.2.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (3.4.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text->trax) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import random as rnd\n",
    "\n",
    "# import relevant libraries\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import fastmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tweet_id  manual score  machine score  sentiment_score  \\\n",
      "0      1.240000e+18             0              1         0.116071   \n",
      "1      1.240000e+18            -1             -1        -0.050000   \n",
      "2      1.240000e+18            -1             -1        -0.714286   \n",
      "3      1.240000e+18             0              1         0.700000   \n",
      "4      1.240000e+18             0              0         0.000000   \n",
      "...             ...           ...            ...              ...   \n",
      "11452  1.270000e+18             0              0         0.000000   \n",
      "11453  1.270000e+18            -1             -1        -0.071429   \n",
      "11454  1.270000e+18            -1             -1        -0.200000   \n",
      "11455  1.270000e+18            -1             -1        -0.243056   \n",
      "11456  1.270000e+18            -1             -1        -0.092593   \n",
      "\n",
      "                                                    text  \\\n",
      "0      Wuhan has been in complete quarantine for over...   \n",
      "1      RT @RedTRaccoon: You can no longer defend this...   \n",
      "2            RT @nicolebyer: Everyone in this looks sick   \n",
      "3           RT @RealSaavedra: Good.  It came from China.   \n",
      "4      RT @Biancaixvi: Corona day 3: it just feels li...   \n",
      "...                                                  ...   \n",
      "11452  RT @Tsinghua_Uni: President Qiu Yong stressed ...   \n",
      "11453  RT @JaniceDean: He will blame every single per...   \n",
      "11454  British theatre has a class problem, and coron...   \n",
      "11455  Check your phones. So supposedly when the phon...   \n",
      "11456  The pandemic-induced lockdown has made matters...   \n",
      "\n",
      "                                 date language  favorite_count month  \n",
      "0      Thu Mar 19 19:52:15 +0000 2020       en              13  Mar   \n",
      "1      Thu Mar 19 19:52:15 +0000 2020       en               0  Mar   \n",
      "2      Thu Mar 19 19:52:15 +0000 2020       en               0  Mar   \n",
      "3      Thu Mar 19 19:52:15 +0000 2020       en               0  Mar   \n",
      "4      Thu Mar 19 19:52:15 +0000 2020       en               0  Mar   \n",
      "...                               ...      ...             ...   ...  \n",
      "11452  Fri Jun 19 04:32:25 +0000 2020       en               0  Jun   \n",
      "11453  Fri Jun 19 04:32:25 +0000 2020       en               0  Jun   \n",
      "11454  Fri Jun 19 04:32:25 +0000 2020       en               0  Jun   \n",
      "11455  Fri Jun 19 04:32:25 +0000 2020       en               0  Jun   \n",
      "11456  Fri Jun 19 04:32:25 +0000 2020       en               4  Jun   \n",
      "\n",
      "[11457 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'covid_tweets_preprocessed-Copy1.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11457, 9)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into pos/neu/neg, sampling at the minimum length == 2689\n",
    "\n",
    "df_positive = df[df['machine score'] == 1].sample(2689)\n",
    "# df_neutral = df[df['machine score'] == 0].sample(2689)\n",
    "df_negative = df[df['machine score'] == -1].sample(2689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positive = df_positive.sample(frac=0.8,random_state=200)\n",
    "test_positive = df_positive.drop(train_positive.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_neutral = df_neutral.sample(frac=0.8,random_state=200)\n",
    "# test_neutral = df_neutral.drop(train_neutral.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_negative = df_negative.sample(frac=0.8,random_state=200)\n",
    "test_negative = df_negative.drop(train_negative.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "import random as rnd\n",
    "\n",
    "# import relevant libraries\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import fastmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_positive, train_negative], axis = 0)\n",
    "test = pd.concat([test_positive, test_negative], axis = 0)\n",
    "\n",
    "# train = pd.concat([train_positive, train_negative], axis = 0)\n",
    "# test = pd.concat([test_positive, test_negative], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>manual score</th>\n",
       "      <th>machine score</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>RT @TamangPhan: Why are people getting judged ...</td>\n",
       "      <td>Thu Mar 19 20:01:16 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6216</th>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>RT @kpnorcal: A 70 year old COVID-19 patient i...</td>\n",
       "      <td>Tue May 19 04:23:49 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>RT @You_grow_wai: Hello Fam üíô Ghana Has Con...</td>\n",
       "      <td>Thu Mar 19 19:58:34 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7878</th>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>RT @pythoroshan: @sudhirchaudhary @ZeeNews Abs...</td>\n",
       "      <td>Tue May 19 04:28:59 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>1.250000e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>RT @jerry_jtaylor: Per @dana1981, COVID-denial...</td>\n",
       "      <td>Sat Apr 18 04:51:06 +0000 2020</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Apr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tweet_id  manual score  machine score  sentiment_score  \\\n",
       "2808  1.240000e+18             0              1         0.125000   \n",
       "6216  1.260000e+18             1              1         0.150000   \n",
       "1942  1.240000e+18             0              1         0.437500   \n",
       "7878  1.260000e+18             1              1         0.150000   \n",
       "3666  1.250000e+18             0              1         0.318182   \n",
       "\n",
       "                                                   text  \\\n",
       "2808  RT @TamangPhan: Why are people getting judged ...   \n",
       "6216  RT @kpnorcal: A 70 year old COVID-19 patient i...   \n",
       "1942  RT @You_grow_wai: Hello Fam üíô Ghana Has Con...   \n",
       "7878  RT @pythoroshan: @sudhirchaudhary @ZeeNews Abs...   \n",
       "3666  RT @jerry_jtaylor: Per @dana1981, COVID-denial...   \n",
       "\n",
       "                                date language  favorite_count month  \n",
       "2808  Thu Mar 19 20:01:16 +0000 2020       en               0  Mar   \n",
       "6216  Tue May 19 04:23:49 +0000 2020       en               0  May   \n",
       "1942  Thu Mar 19 19:58:34 +0000 2020       en               0  Mar   \n",
       "7878  Tue May 19 04:28:59 +0000 2020       en               0  May   \n",
       "3666  Sat Apr 18 04:51:06 +0000 2020       en               0  Apr   "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets():\n",
    "    # Load positive and negative tweets\n",
    "    df = pd.read_csv(r'covid_tweets_preprocessed-Copy1.csv')\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    df_positive = df[df['manual score'] == 1].sample(2689)\n",
    "#     df_neutral = df[df['manual score'] == 0].sample(2689)\n",
    "    df_negative = df[df['manual score'] == -1].sample(2689)\n",
    "    \n",
    "    df_train_positive = df_positive.sample(frac=0.8,random_state=200)\n",
    "    df_test_positive = df_positive.drop(df_train_positive.index)\n",
    "    \n",
    "#     df_train_neutral = df_neutral.sample(frac=0.8,random_state=200)\n",
    "#     df_test_neutral = df_neutral.drop(df_train_neutral.index)\n",
    "    \n",
    "    df_train_negative = df_negative.sample(frac=0.8,random_state=200)\n",
    "    df_test_negative = df_negative.drop(df_train_negative.index)\n",
    "    \n",
    "    train_pos = list(df_train_positive['text'])\n",
    "    test_pos = list(df_test_positive['text'])\n",
    "    \n",
    "#     train_neu = list(df_train_neutral['text'])\n",
    "#     test_neu = list(df_test_neutral['text'])   \n",
    "    \n",
    "    train_neg = list(df_train_negative['text'])\n",
    "    test_neg = list(df_test_negative['text'])\n",
    "    \n",
    "    # Combine training data into one set\n",
    "#     train_x = train_pos + train_neu + train_neg \n",
    "    train_x = train_pos + train_neg \n",
    "\n",
    "    # Combine testidation data into one set\n",
    "#     test_x  = test_pos + test_neu + test_neg\n",
    "    test_x  = test_pos + test_neg\n",
    "\n",
    "    # Set the labels for the training set (1 for positive, 0 for neutral, -1 for negative)\n",
    "#     train_y = np.append(np.append(np.ones(len(train_pos)), np.zeros(len(train_neu))), -1*np.ones(len(train_neg)))\n",
    "    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "    \n",
    "    # Set the labels for the test set (1 for positive, 0 for negative, -1 for negative)\n",
    "#     train_y = np.append(np.append(np.ones(len(test_pos)), np.zeros(len(test_neu))), -1*np.ones(len(test_neg)))\n",
    "    test_y  = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
    "\n",
    "\n",
    "    return train_pos, train_neg, train_x, train_y, test_pos, test_neg, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos, train_neg, train_x, train_y, test_pos, test_neg, test_x, test_y = load_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 1., 1., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \n",
    "    '''\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https.*?\\s', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    ### START CODE HERE ###\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    ### END CODE HERE ###\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Vocabualary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['remind', 'anyon', 'certain', 'pm', 'name', 'scumo']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(train_x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<class 'str'>}\n"
     ]
    }
   ],
   "source": [
    "type(train_x)\n",
    "ls = {type(item) for item in train_x}\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(train_x):\n",
    "\n",
    "    # Include special tokens \n",
    "    # started with pad, end of line and unk tokens\n",
    "    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "    # Note that we build vocab using training data\n",
    "    for tweet in train_x: \n",
    "        processed_tweet = process_tweet(tweet)\n",
    "        for word in processed_tweet:\n",
    "            if word not in Vocab: \n",
    "                Vocab[word] = len(Vocab)\n",
    "    \n",
    "    return Vocab\n",
    "\n",
    "Vocab = get_vocab(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8823"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert words to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''     \n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    tensor_l = [] \n",
    "    \n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    for word in word_l:\n",
    "\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "        tensor_l.append(word_ID)\n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " RT @MaryanneChisho2: Hi everyone, just an update - My daughters COVID-19 test was negative. We are relieved!\n",
      "\n",
      "Tensor of tweet:\n",
      " [1946, 1403, 1720, 40, 31, 81, 13, 4359, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", test_x[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(test_x[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_generator(data_pos, data_neu, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "#     '''\n",
    "#     Input: \n",
    "#         data_pos - Set of positive examples\n",
    "#         data_neu - Set of neutral examples\n",
    "#         data_neg - Set of negative examples\n",
    "#         batch_size - number of samples per batch. Must be a multiple of three\n",
    "#         loop - True or False\n",
    "#         vocab_dict - The words dictionary\n",
    "#         shuffle - Shuffle the data order\n",
    "#     Yield:\n",
    "#         inputs - Subset of positive and negative examples\n",
    "#         targets - The corresponding labels for the subset\n",
    "#         example_weights - An array specifying the importance of each example\n",
    "        \n",
    "#     '''     \n",
    "\n",
    "#     # make sure the batch size is a multiple of three\n",
    "#     # to allow an equal number of positive and negative samples    \n",
    "#     assert batch_size % 3 == 0\n",
    "    \n",
    "#     # Number of positive examples in each batch is half of the batch size\n",
    "#     # same with number of negative examples in each batch\n",
    "#     n_to_take = batch_size // 3\n",
    "    \n",
    "#     # Use pos_index to walk through the data_pos array\n",
    "#     # same with neg_index and data_neg\n",
    "#     pos_index = 0\n",
    "#     neu_index\n",
    "#     neg_index = 0\n",
    "    \n",
    "#     len_data_pos = len(data_pos)\n",
    "#     len_data_neu = len(data_neu)\n",
    "#     len_data_neg = len(data_neg)\n",
    "    \n",
    "#     # Get and array with the data indexes\n",
    "#     pos_index_lines = list(range(len_data_pos))\n",
    "#     neu_index_lines = list(range(len_data_neu))\n",
    "#     neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "#     # shuffle lines if shuffle is set to True\n",
    "#     if shuffle:\n",
    "#         rnd.shuffle(pos_index_lines)\n",
    "#         rnd.shuffle(neu_index_lines)\n",
    "#         rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "#     stop = False\n",
    "    \n",
    "#     # Loop indefinitely\n",
    "#     while not stop:  \n",
    "        \n",
    "#         # create a batch with positive, neutral and negative examples\n",
    "#         batch = []\n",
    "        \n",
    "#         # First part: Pack n_to_take positive examples\n",
    "        \n",
    "#         # Start from 0 and increment i up to n_to_take\n",
    "#         for i in range(n_to_take):\n",
    "                    \n",
    "#             # If the positive index goes past the positive dataset,\n",
    "#             if pos_index >= len_data_pos: \n",
    "                \n",
    "#                 # If loop is set to False, break once we reach the end of the dataset\n",
    "#                 if not loop:\n",
    "#                     stop = True;\n",
    "#                     break;\n",
    "#                 # If user wants to keep re-using the data, reset the index\n",
    "#                 pos_index = 0\n",
    "#                 if shuffle:\n",
    "#                     # Shuffle the index of the positive sample\n",
    "#                     rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "#             # get the tweet as pos_index\n",
    "#             tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "#             # convert the tweet into tensors of integers representing the processed words\n",
    "#             tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "#             # append the tensor to the batch list\n",
    "#             batch.append(tensor)\n",
    "            \n",
    "#             # Increment pos_index by one\n",
    "#             pos_index = pos_index + 1\n",
    "            \n",
    "#         for i in range(n_to_take):\n",
    "\n",
    "#             if neu_index >= len_data_neu:\n",
    "                \n",
    "#                 if not loop:\n",
    "#                     stop = True;\n",
    "#                     break;\n",
    "                    \n",
    "#                 neu_index = 0\n",
    "#                 if shuffle:\n",
    "#                     rnd.shuffle(neu_index_lines)\n",
    "                    \n",
    "#             tweet = data_neu[neu_index_lines[neu_index]]\n",
    "            \n",
    "#         ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "#         # Second part: Pack n_to_take negative examples\n",
    "\n",
    "#         # Using the same batch list, start from 0 and increment i up to n_to_take\n",
    "#         for i in range(n_to_take):\n",
    "            \n",
    "#             # If the negative index goes past the negative dataset,\n",
    "#             if neg_index >= len_data_neg:\n",
    "                \n",
    "#                 # If loop is set to False, break once we reach the end of the dataset\n",
    "#                 if not loop:\n",
    "#                     stop = True \n",
    "#                     break \n",
    "                    \n",
    "#                 # If user wants to keep re-using the data, reset the index\n",
    "#                 neg_index = 0\n",
    "                \n",
    "#                 if shuffle:\n",
    "#                     # Shuffle the index of the negative sample\n",
    "#                     rnd.shuffle(neg_index_lines)\n",
    "                    \n",
    "#             # get the tweet as neg_index\n",
    "#             tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "#             # convert the tweet into tensors of integers representing the processed words\n",
    "#             tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "#             # append the tensor to the batch list\n",
    "#             batch.append(tensor)\n",
    "            \n",
    "#             # Increment neg_index by one\n",
    "#             neg_index += 1\n",
    "\n",
    "#         ### END CODE HERE ###        \n",
    "\n",
    "#         if stop:\n",
    "#             break;\n",
    "\n",
    "#         # Get the max tweet length (the length of the longest tweet) \n",
    "#         # (you will pad all shorter tweets to have this length)\n",
    "#         max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "#         # Initialize the input_l, which will \n",
    "#         # store the padded versions of the tensors\n",
    "#         tensor_pad_l = []\n",
    "#         # Pad shorter tweets with zeros\n",
    "#         for tensor in batch:\n",
    "\n",
    "\n",
    "#         ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "#             # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "#             n_pad = max_len - len(tensor)\n",
    "            \n",
    "#             # Generate a list of zeros, with length n_pad\n",
    "#             pad_l = [0]*n_pad\n",
    "            \n",
    "#             # concatenate the tensor and the list of padded zeros\n",
    "#             tensor_pad = tensor + pad_l\n",
    "            \n",
    "#             # append the padded tensor to the list of padded tensors\n",
    "#             tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "#         # convert the list of padded tensors to a numpy array\n",
    "#         # and store this as the model inputs\n",
    "#         inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "#         # Generate the list of targets for the positive examples (a list of ones)\n",
    "#         # The length is the number of positive examples in the batch\n",
    "#         target_pos = [1]*n_to_take\n",
    "        \n",
    "#         # Generate the list of targets for the neutral examples (a list of zeros)\n",
    "#         # The length is the number of positive examples in the batch\n",
    "        \n",
    "#         target_neu = [0]*n_to_take\n",
    "        \n",
    "#         # Generate the list of targets for the negative examples (a list of negative ones)\n",
    "#         # The length is the number of negative examples in the batch\n",
    "#         target_neg = [-1]*n_to_take\n",
    "        \n",
    "#         # Concatenate the positve and negative targets\n",
    "#         target_l = target_pos + target_neu + target_neg\n",
    "        \n",
    "#         # Convert the target list into a numpy array\n",
    "#         targets = np.array(target_l)\n",
    "\n",
    "#         # Example weights: Treat all examples equally importantly.\n",
    "#         example_weights = np.ones_like(targets)\n",
    "        \n",
    "\n",
    "#         ### END CODE HERE ###\n",
    "\n",
    "#         # note we use yield and not return\n",
    "#         yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "        data_pos - Set of positive examples\n",
    "        data_neu - Set of neutral examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch. Must be a multiple of three\n",
    "        loop - True or False\n",
    "        vocab_dict - The words dictionary\n",
    "        shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "        inputs - Subset of positive and negative examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - An array specifying the importance of each example\n",
    "        \n",
    "    '''     \n",
    "\n",
    "    # make sure the batch size is a multiple of three\n",
    "    # to allow an equal number of positive and negative samples    \n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk through the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "#     neu_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "#     len_data_neu = len(data_neu)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "#     neu_index_lines = list(range(len_data_neu))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "#         rnd.shuffle(neu_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        \n",
    "        # create a batch with positive, neutral and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        \n",
    "        # Start from 0 and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            # If the positive index goes past the positive dataset,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "            \n",
    "#         for i in range(n_to_take):\n",
    "\n",
    "#             if neu_index >= len_data_neu:\n",
    "                \n",
    "#                 if not loop:\n",
    "#                     stop = True;\n",
    "#                     break;\n",
    "                    \n",
    "#                 neu_index = 0\n",
    "#                 if shuffle:\n",
    "#                     rnd.shuffle(neu_index_lines)\n",
    "                    \n",
    "#             tweet = data_neu[neu_index_lines[neu_index]]\n",
    "            \n",
    "#             # convert the tweet into tensors of integers representing the processed words\n",
    "#             tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "#             # append the tensor to the batch list\n",
    "#             batch.append(tensor)\n",
    "            \n",
    "#             # Increment pos_index by one\n",
    "#             neu_index = neu_index + 1\n",
    "            \n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "\n",
    "        # Using the same batch list, start from 0 and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            # If the negative index goes past the negative dataset,\n",
    "            if neg_index >= len_data_neg:\n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True \n",
    "                    break \n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "                    \n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment neg_index by one\n",
    "            neg_index += 1\n",
    "\n",
    "        ### END CODE HERE ###        \n",
    "\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "\n",
    "\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = [0]*n_pad\n",
    "            \n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = [1]*n_to_take\n",
    "        \n",
    "        # Generate the list of targets for the neutral examples (a list of zeros)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        \n",
    "        target_neg = [0]*n_to_take\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of negative ones)\n",
    "        # The length is the number of negative examples in the batch\n",
    "#         target_neg = [-1]*n_to_take\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "#         target_l = target_pos + target_neu + target_neg\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the random number generator for the shuffle procedure\n",
    "# rnd.seed(30) \n",
    "\n",
    "# # Create the training data generator\n",
    "\n",
    "# def train_generator(batch_size, train_pos, train_neu, train_neg, vocab_dict, loop=True, shuffle = False):\n",
    "#     return data_generator(train_pos, train_neu, train_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# # Create the testidation data generator\n",
    "# def test_generator(batch_size, test_pos, test_neu\n",
    "#                     , test_neg, vocab_dict, loop=True\n",
    "#                     , shuffle = False):\n",
    "#     return data_generator(test_pos, test_neu, test_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# # Create the testidation data generator\n",
    "# def test_generator(batch_size, test_pos, test_neu\n",
    "#                     , test_neg, vocab_dict, loop=False\n",
    "#                     , shuffle = False):\n",
    "#     return data_generator(test_pos, test_neu, test_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# # Get a batch from the train_generator and inspect.\n",
    "# inputs, targets, example_weights = next(train_generator(6, train_pos, train_neu, train_neg, Vocab, shuffle=True))\n",
    "\n",
    "# # this will print a list of 4 tensors padded with zeros\n",
    "# print(f'Inputs: {inputs}')\n",
    "# print(f'Targets: {targets}')\n",
    "# print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[ 266  260  267  647  270 2224 3720 1587  266  260 3248  502 1913  827\n",
      "   703  125 3721  895  238 1911   68  263 3722  234  404  392   35  127\n",
      "  3723]\n",
      " [ 383  111 1262  168  266 1264 1265 1264  383   54   17   19   20    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [  90 6236   90 2066 6236 5971 1457 5677    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [  29  646 6458 3638 6285 5845 6458 4590 1703 6459 6254 1211   17   19\n",
      "    20    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "\n",
    "def train_generator(batch_size, train_pos, train_neg, vocab_dict, loop=True, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Create the testidation data generator\n",
    "def test_generator(batch_size, test_pos\n",
    "                    , test_neg, vocab_dict, loop=True\n",
    "                    , shuffle = False):\n",
    "    return data_generator(test_pos, test_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Create the testidation data generator\n",
    "def test_generator(batch_size, test_pos\n",
    "                    , test_neg, vocab_dict, loop=False\n",
    "                    , shuffle = False):\n",
    "    return data_generator(test_pos, test_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, train_pos, train_neg, Vocab, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [[1187 1188 1189 1190   13   31   81  142 1191 1192 1193 1194 1195    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  31   81   13 3209 1228 1720  798  472 3210   81 1409    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 332  333  334  335  336    7  290    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [4367 7012  745  572  200   17  422 7013  190 1515   17  422 7013  190\n",
      "  1515 7014 3256 1556 2365  524 6502 6675   17   19   20]\n",
      " [1100   62 1327 2057   47  221  628 6338  497 2736   17   19   20    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  29  299 1652  517 6479 1385 6480    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 1 0 0 0]\n",
      "Example Weights: [1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, example_weights = next(train_generator(6, train_pos, train_neg, Vocab, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs shape is (6, 21)\n",
      "input tensor: [ 3  4  5  6  7  8  7  9 10 11 12 13 14 13 15 16 17 18 17 19 20]; target 1; example weights 1\n",
      "input tensor: [21 22 23 24 25 26 27 28 29 30 31 32 33 25 22 34 35 36  0  0  0]; target 1; example weights 1\n",
      "input tensor: [37 29 38 39 40 35 41 42 43  0  0  0  0  0  0  0  0  0  0  0  0]; target 1; example weights 1\n",
      "input tensor: [  17  251 5430 1389  238  642  118    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 820  342  171 4305   99 1545  412   31    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [  90 6236   90 2066 6236 5971 1457 5677    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generator\n",
    "\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective targets)\n",
    "tmp_data_gen = train_generator(batch_size = 6, train_pos=train_pos, train_neg=train_neg, vocab_dict=Vocab)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\" Base class for layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # set weights to None\n",
    "        self.weights = None\n",
    "\n",
    "    # The forward propagation should be implemented\n",
    "    # by subclasses of this Layer class\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # This function initializes the weights\n",
    "    # based on the input signature and random key,\n",
    "    # should be implemented by subclasses of this Layer class\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        pass\n",
    "\n",
    "    # This initializes and returns the weights, do not override.\n",
    "    def init(self, input_signature, random_key):\n",
    "        self.init_weights_and_state(input_signature, random_key)\n",
    "        return self.weights\n",
    "\n",
    "    # __call__ allows an object of this class\n",
    "    # to be called like it's a function.\n",
    "    def __call__(self, x):\n",
    "        # When this layer object is called, \n",
    "        # it calls its forward propagation function\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Layer):\n",
    "    \"\"\"Relu activation function implementation\"\"\"\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input: \n",
    "            - x (a numpy array): the input\n",
    "        Output:\n",
    "            - activation (numpy array): all positive or 0 version of x\n",
    "        '''\n",
    "        activation = np.maximum(x,0)\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    A dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # __init__ is implemented for you\n",
    "    def __init__(self, n_units, init_stdev=0.1):\n",
    "        \n",
    "        # Set the number of units in this layer\n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = init_stdev\n",
    "\n",
    "    # Please implement 'forward()'\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Matrix multiply x and the weight matrix\n",
    "        dense = np.dot(x, self.weights)\n",
    "\n",
    "        return dense\n",
    "\n",
    "    # init_weights\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        \n",
    "        input_shape = input_signature.shape\n",
    "       \n",
    "        w = self._init_stdev * trax.fastmath.random.normal(key = random_key, shape = (input_shape[-1], self._n_units))\n",
    " \n",
    "        self.weights = w\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(vocab_size=9914, embedding_dim=256, output_dim=2, mode='train'):\n",
    "\n",
    "    # create embedding layer\n",
    "    embed_layer = tl.Embedding( \n",
    "        vocab_size=vocab_size, # Size of the vocabulary\n",
    "        d_feature=embedding_dim # Embedding dimension\n",
    "    ) \n",
    "    \n",
    "    # Create a mean layer, to create an \"average\" word embedding\n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    \n",
    "    # Create a dense layer, one unit for each output\n",
    "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
    "    \n",
    "    # Create the log softmax layer (no parameters needed)\n",
    "    log_softmax_layer = tl.LogSoftmax()\n",
    "    \n",
    "    # Use tl.Serial to combine all layers\n",
    "    # and create the classifier\n",
    "    # of type trax.layers.combinators.Serial\n",
    "    model = tl.Serial( \n",
    "      embed_layer, # embedding layer\n",
    "      mean_layer, # mean layer\n",
    "      dense_output_layer, # dense output layer\n",
    "      log_softmax_layer, # log softmax layer\n",
    "    ) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # return the model of type\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "def get_train_eval_tasks(train_pos, train_neg, test_pos, test_neg, vocab_dict, loop, batch_size = 16):\n",
    "    \n",
    "    rnd.seed(271)\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator(batch_size, train_pos\n",
    "                    , train_neg, vocab_dict, loop\n",
    "                    , shuffle = True),\n",
    "        loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        n_steps_per_checkpoint=10,\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=test_generator(batch_size, test_pos\n",
    "                    , test_neg, vocab_dict, loop\n",
    "                    , shuffle = True),        \n",
    "        metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n",
    "    )\n",
    "    \n",
    "    return train_task, eval_task\n",
    "    \n",
    "\n",
    "train_task, eval_task = get_train_eval_tasks(train_pos, train_neg, test_pos, test_neg, Vocab, True, batch_size = 24)\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_9914_256\n",
       "  Mean\n",
       "  Dense_2\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/\n"
     ]
    }
   ],
   "source": [
    "dir_path = './model/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    pass\n",
    "\n",
    "\n",
    "output_dir = './model/'\n",
    "output_dir_expand = os.path.expanduser(output_dir)\n",
    "print(output_dir_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    '''\n",
    "    Input: \n",
    "        classifier - the model you are building\n",
    "        train_task - Training task\n",
    "        eval_task - Evaluation task. Received as a list.\n",
    "        n_steps - the evaluation steps\n",
    "        output_dir - folder to save your files\n",
    "    Output:\n",
    "        trainer -  trax trainer\n",
    "    '''\n",
    "    rnd.seed(31) # Do NOT modify this random seed. This makes the notebook easier to replicate\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###          \n",
    "    training_loop = training.Loop( \n",
    "                                classifier, # The learning model\n",
    "                                train_task, # The training task\n",
    "                                eval_tasks=eval_task, # The evaluation task\n",
    "                                output_dir=output_dir, # The output directory\n",
    "                                random_seed=31 # Do not modify this random seed in order to ensure reproducibility and for grading purposes.\n",
    "    ) \n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2538498\n",
      "Step      1: Ran 1 train steps in 1.18 secs\n",
      "Step      1: train WeightedCategoryCrossEntropy |  0.33064303\n",
      "Step      1: eval  WeightedCategoryCrossEntropy |  0.46775231\n",
      "Step      1: eval      WeightedCategoryAccuracy |  0.79166669\n",
      "\n",
      "Step     10: Ran 9 train steps in 3.29 secs\n",
      "Step     10: train WeightedCategoryCrossEntropy |  0.43946570\n",
      "Step     10: eval  WeightedCategoryCrossEntropy |  0.50949985\n",
      "Step     10: eval      WeightedCategoryAccuracy |  0.83333331\n",
      "\n",
      "Step     20: Ran 10 train steps in 3.21 secs\n",
      "Step     20: train WeightedCategoryCrossEntropy |  0.42646426\n",
      "Step     20: eval  WeightedCategoryCrossEntropy |  0.38794735\n",
      "Step     20: eval      WeightedCategoryAccuracy |  0.75000000\n",
      "\n",
      "Step     30: Ran 10 train steps in 3.72 secs\n",
      "Step     30: train WeightedCategoryCrossEntropy |  0.39143488\n",
      "Step     30: eval  WeightedCategoryCrossEntropy |  0.55485648\n",
      "Step     30: eval      WeightedCategoryAccuracy |  0.79166669\n",
      "\n",
      "Step     40: Ran 10 train steps in 3.28 secs\n",
      "Step     40: train WeightedCategoryCrossEntropy |  0.39014378\n",
      "Step     40: eval  WeightedCategoryCrossEntropy |  0.17179181\n",
      "Step     40: eval      WeightedCategoryAccuracy |  0.91666669\n",
      "\n",
      "Step     50: Ran 10 train steps in 2.47 secs\n",
      "Step     50: train WeightedCategoryCrossEntropy |  0.44834027\n",
      "Step     50: eval  WeightedCategoryCrossEntropy |  0.40153837\n",
      "Step     50: eval      WeightedCategoryAccuracy |  0.75000000\n",
      "\n",
      "Step     60: Ran 10 train steps in 1.70 secs\n",
      "Step     60: train WeightedCategoryCrossEntropy |  0.40506166\n",
      "Step     60: eval  WeightedCategoryCrossEntropy |  0.40435037\n",
      "Step     60: eval      WeightedCategoryAccuracy |  0.79166669\n",
      "\n",
      "Step     70: Ran 10 train steps in 1.77 secs\n",
      "Step     70: train WeightedCategoryCrossEntropy |  0.43071651\n",
      "Step     70: eval  WeightedCategoryCrossEntropy |  0.33444336\n",
      "Step     70: eval      WeightedCategoryAccuracy |  0.79166669\n",
      "\n",
      "Step     80: Ran 10 train steps in 1.76 secs\n",
      "Step     80: train WeightedCategoryCrossEntropy |  0.34286800\n",
      "Step     80: eval  WeightedCategoryCrossEntropy |  0.41685855\n",
      "Step     80: eval      WeightedCategoryAccuracy |  0.70833331\n",
      "\n",
      "Step     90: Ran 10 train steps in 1.74 secs\n",
      "Step     90: train WeightedCategoryCrossEntropy |  0.17221537\n",
      "Step     90: eval  WeightedCategoryCrossEntropy |  0.20531285\n",
      "Step     90: eval      WeightedCategoryAccuracy |  0.91666669\n",
      "\n",
      "Step    100: Ran 10 train steps in 1.38 secs\n",
      "Step    100: train WeightedCategoryCrossEntropy |  0.21701467\n",
      "Step    100: eval  WeightedCategoryCrossEntropy |  0.20514937\n",
      "Step    100: eval      WeightedCategoryAccuracy |  0.91666669\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(model, train_task, [eval_task], 100, output_dir_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
      "The shape of the tweet tensors is (16, 28) (num of examples, length of tweet tensors)\n",
      "The shape of the labels is (16,), which is the batch size.\n",
      "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
     ]
    }
   ],
   "source": [
    "# Create a generator object\n",
    "tmp_train_generator = train_generator(16, train_pos\n",
    "                    , train_neg, Vocab, loop=True\n",
    "                    , shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, y, y_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        preds: a tensor of shape (dim_batch, output_dim) \n",
    "        y: a tensor of shape (dim_batch,) with the true labels\n",
    "        y_weights: a n.ndarray with the a weight for each example\n",
    "    Output: \n",
    "        accuracy: a float between 0-1 \n",
    "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
    "        sum_weights (np.float32): Sum of the weights\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Create an array of booleans, \n",
    "    # True if the probability of positive sentiment is greater than\n",
    "    # the probability of negative sentiment\n",
    "    # else False\n",
    "    is_pos = preds[:, 1] > preds[:, 0] \n",
    "\n",
    "    # convert the array of booleans into an array of np.int32\n",
    "    is_pos_int = is_pos.astype(np.int32)\n",
    "    \n",
    "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
    "    correct = is_pos_int == y\n",
    "\n",
    "    # Count the sum of the weights.\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    \n",
    "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
    "    correct_float = correct.astype(np.float32)\n",
    "    \n",
    "    # Multiply each prediction with its corresponding weight.\n",
    "    weighted_correct_float = correct_float * y_weights\n",
    "\n",
    "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
    "    # denominator.\n",
    "    weighted_num_correct = np.sum(weighted_correct_float)\n",
    "\n",
    "    # Divide the number of weighted correct predictions by the sum of the\n",
    "    # weights.\n",
    "    accuracy = weighted_num_correct / sum_weights\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
      "Column 0 is the probability of a negative sentiment (class 0)\n",
      "Column 1 is the probability of a positive sentiment (class 1)\n",
      "\n",
      "View the prediction array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-7.7691083e+00, -4.2271614e-04],\n",
       "             [-5.5346572e-01, -8.5555720e-01],\n",
       "             [-1.9592929e+00, -1.5193754e-01],\n",
       "             [-2.1556168e+00, -1.2310791e-01],\n",
       "             [-2.0442739e+00, -1.3865787e-01],\n",
       "             [-3.7320912e-01, -1.1664240e+00],\n",
       "             [-4.8675847e+00, -7.7216625e-03],\n",
       "             [-7.9531021e+00, -3.5166740e-04],\n",
       "             [-3.7326813e-03, -5.5924702e+00],\n",
       "             [-3.1380653e-03, -5.7657194e+00],\n",
       "             [-7.5914860e-03, -4.8845310e+00],\n",
       "             [-3.7376881e-03, -5.5911465e+00],\n",
       "             [-4.8426688e-01, -9.5750010e-01],\n",
       "             [-1.9037724e-03, -6.2647991e+00],\n",
       "             [-1.0868633e-01, -2.2731400e+00],\n",
       "             [-1.1870861e-03, -6.7368240e+00]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg log prob -7.7691\tPos log prob -0.0004\t is positive? True\t actual 1\n",
      "Neg log prob -0.5535\tPos log prob -0.8556\t is positive? False\t actual 1\n",
      "Neg log prob -1.9593\tPos log prob -0.1519\t is positive? True\t actual 1\n",
      "Neg log prob -2.1556\tPos log prob -0.1231\t is positive? True\t actual 1\n",
      "Neg log prob -2.0443\tPos log prob -0.1387\t is positive? True\t actual 1\n",
      "Neg log prob -0.3732\tPos log prob -1.1664\t is positive? False\t actual 1\n",
      "Neg log prob -4.8676\tPos log prob -0.0077\t is positive? True\t actual 1\n",
      "Neg log prob -7.9531\tPos log prob -0.0004\t is positive? True\t actual 1\n",
      "Neg log prob -0.0037\tPos log prob -5.5925\t is positive? False\t actual 0\n",
      "Neg log prob -0.0031\tPos log prob -5.7657\t is positive? False\t actual 0\n",
      "Neg log prob -0.0076\tPos log prob -4.8845\t is positive? False\t actual 0\n",
      "Neg log prob -0.0037\tPos log prob -5.5911\t is positive? False\t actual 0\n",
      "Neg log prob -0.4843\tPos log prob -0.9575\t is positive? False\t actual 0\n",
      "Neg log prob -0.0019\tPos log prob -6.2648\t is positive? False\t actual 0\n",
      "Neg log prob -0.1087\tPos log prob -2.2731\t is positive? False\t actual 0\n",
      "Neg log prob -0.0012\tPos log prob -6.7368\t is positive? False\t actual 0\n"
     ]
    }
   ],
   "source": [
    "# turn probabilites into category predictions\n",
    "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's prediction accuracy on a single training batch is: 100.0%\n",
      "Weighted number of correct predictions 16.0; weighted number of total observations predicted 16\n"
     ]
    }
   ],
   "source": [
    "tmp_val_generator = train_generator(64, train_pos\n",
    "                    , train_neg, Vocab, loop=True\n",
    "                    , shuffle = False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
    "\n",
    "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
    "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(generator, model, compute_accuracy=compute_accuracy):\n",
    "    '''\n",
    "    Input: \n",
    "        generator: an iterator instance that provides batches of inputs and targets\n",
    "        model: a model instance \n",
    "    Output: \n",
    "        accuracy: float corresponding to the accuracy\n",
    "    '''\n",
    "    \n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "        \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    for batch in generator: \n",
    "        \n",
    "        # Retrieve the inputs from the batch\n",
    "        inputs = batch[0]\n",
    "        \n",
    "        # Retrieve the targets (actual labels) from the batch\n",
    "        targets = batch[1]\n",
    "        \n",
    "        # Retrieve the example weight.\n",
    "        example_weight = batch[2]\n",
    "\n",
    "        # Make predictions using the inputs            \n",
    "        pred = model(inputs)\n",
    "        \n",
    "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)\n",
    "                \n",
    "        # Update the total number of correct predictions\n",
    "        # by adding the number of correct predictions from this batch\n",
    "        total_num_correct += batch_num_correct\n",
    "        \n",
    "        # Update the total number of predictions \n",
    "        # by adding the number of predictions made for the batch\n",
    "        total_num_pred += batch_num_pred\n",
    "\n",
    "    # Calculate accuracy over all examples\n",
    "    accuracy = total_num_correct / total_num_pred\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of your model on the validation set is 0.8153\n"
     ]
    }
   ],
   "source": [
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16, test_pos\n",
    "                    , test_neg, Vocab, loop=False\n",
    "                    , shuffle = False), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
